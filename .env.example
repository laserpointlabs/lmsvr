# Ollama API Gateway Environment Configuration
# Copy this file to .env and update with your values
# cp .env.example .env

# ==============================================================================
# REQUIRED SETTINGS
# ==============================================================================

# Database connection string
# Format: sqlite:///path/to/database.db
# For Docker: Use absolute path inside container (e.g., /app/data/lmapi.db)
# For local CLI: Use relative path (e.g., ./data/lmapi.db)
DATABASE_URL=sqlite:///./data/lmapi.db

# Ollama service base URL
# For Docker Compose: http://ollama:11434 (uses Docker network service name)
# For local development: http://localhost:11434
# For remote Ollama: http://your-ollama-server:11434
OLLAMA_BASE_URL=http://ollama:11434

# ==============================================================================
# OPTIONAL SETTINGS
# ==============================================================================

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR
# Default: INFO
LOG_LEVEL=INFO

# OpenAI API key for pass-through endpoints (/v1/chat/completions)
# Leave empty if not using OpenAI endpoints
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude) API key for pass-through endpoints (/v1/messages)
# Leave empty if not using Claude endpoints
# Get your key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Cloudflare Tunnel public URL
# This is the public URL where your API Gateway will be accessible
# Default: https://lmapi.laserpointlabs.com
# After changing this, run: ./cloudflare/update_config_from_env.sh
CLOUDFLARE_TUNNEL_URL=https://lmapi.laserpointlabs.com

# Maximum number of Ollama models to keep loaded in memory concurrently
# Default: 4 (or 3 * number of GPUs if GPU is available)
# Increase this value if you want to keep more models loaded simultaneously
# Note: Each model consumes RAM/VRAM based on its size and quantization level
# Example values:
#   - Small models (1B-3B): 6-8 models
#   - Medium models (7B-13B): 3-4 models
#   - Large models (30B+): 1-2 models
OLLAMA_MAX_LOADED_MODELS=4

# Keep models loaded indefinitely (prevents automatic unloading)
# Default: 5m (models unload after 5 minutes of inactivity)
# Set to -1 or -1m to keep models loaded permanently (forever)
# Set to a positive duration (e.g., 10m, 1h) to extend the timeout
# Negative values keep models loaded until manually unloaded or container restarts
OLLAMA_KEEP_ALIVE=-1

# Comma-separated list of models to preload on Ollama startup
# Models will be automatically pulled (if not available) and loaded permanently
# Example: "llama3.2:latest,qwen3-coder:30b,mistral:latest"
# Leave empty to skip automatic model loading
OLLAMA_PRELOAD_MODELS=llama3.1:8b,qwen2.5-coder:1.5b-base,nomic-embed-text,deepseek-coder:6.7b-base


# ==============================================================================
# EXAMPLES FOR DIFFERENT ENVIRONMENTS
# ==============================================================================

# Example: Local development (no Docker)
# DATABASE_URL=sqlite:///./data/lmapi.db
# OLLAMA_BASE_URL=http://localhost:11434
# LOG_LEVEL=DEBUG

# Example: Docker Compose (default)
# DATABASE_URL=sqlite:////app/data/lmapi.db
# OLLAMA_BASE_URL=http://ollama:11434
# LOG_LEVEL=INFO

# Example: Production with GPU and multiple models
# DATABASE_URL=sqlite:////app/data/lmapi.db
# OLLAMA_BASE_URL=http://ollama:11434
# LOG_LEVEL=WARNING
# OLLAMA_MAX_LOADED_MODELS=6
# CLOUDFLARE_TUNNEL_URL=https://api.yourdomain.com

# Example: With external API keys enabled
# OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Gmail credentials for sending API keys via email (CLI feature)
# Required if using: python cli/cli.py generate-key <customer_id> --send-email
# 
# Setup Instructions:
# 1. Enable 2-Step Verification on your Google account
# 2. Generate App Password: https://myaccount.google.com/apppasswords
#    - Select "Mail" → "Other (Custom name)" → Name it "Bet Assistant CLI"
#    - Copy the 16-character password (no spaces)
# 3. Use your Gmail address and the App Password below
#
# Note: Use App Password, NOT your regular Gmail password
# App Passwords are more secure and can be revoked individually
GMAIL_USER=your-email@gmail.com
GMAIL_PASSWORD=your-16-character-app-password-here

# Default model for Bet Frontend chat interface
# Examples: llama3.2:1b, mistral, qwen3-coder:30b
BET_DEFAULT_MODEL=llama3.2:1b
